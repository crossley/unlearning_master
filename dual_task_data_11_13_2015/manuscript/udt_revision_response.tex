\documentclass[10pt,a4paper]{article} \usepackage{amsmath} \usepackage{parskip}
\usepackage{graphicx} \usepackage{float} \floatstyle{boxed}
\restylefloat{figure}

\begin{document}

\section{Response to Editor}
\subsection{} \textbf{Give the paper a much more self-contained and concise
format as a Research Report. Your writing must reflect the fact that the study
is a conceptual and empirical addition to Crossley et al. (2013), and thus it
should make explicit all the arguments needed to understand the model and its
predictions, but it should also exclude all the extra information that is not
essential to understand the motivation of the study (such as the figures
corresponding to already published results).}

\subsection{} \textbf{Argue convincingly, if it is at all possible, about the
extent to which the present study is consistent with your own predictions. If I
understand something about the rationale of your study, it is aimed at
understanding why relearning a procedural task after having experienced a period
of extinction proceeds faster than the first learning process. However, if the
results observed in this procedure just fail to show those savings, then, no
matter how eloquently you could speculate about the role of fatigue in dual-task
conditions, I guess that this just shows that the procedure has failed. My
recommendation in this case would be to go back to the lab, remove the sources
of this contaminating fatigue, and conduct better experiments.}

\subsection{} \textbf{Revise procedure and analyses: In addition to facing the
most serious concern about the lack of savings and its potential causes, there
is also another couple of problems concerning the dual-task procedure and the
statistical analyses. As for the dual task, I'm worried that you might have
included in your sample a large series of participants who performed the Stroop
task only slightly better than expected by chance. As judging from the
histograms, there were many participants in your samples who performed below
80\% of accuracy, which could be taken as a reasonable limit if participants are
really complying with the described instructions, and there is still a number of
participants who performed at chance. This is plainly unacceptable if you want
to assume that participants are performing this secondary task.}

\subsection{} \textbf{Finally, you must exhaustively revise the statistical
analyses reported in the study, especially with respect to the reported the
degrees of freedom. I failed to understand how you conducted your statistical
analyses, but my computation of the degrees of freedom corresponding to the
reported designs is very different from those reported in the manuscript. I'm
afraid that your inferences may be unwarranted if your computations are
mistaken.}

\section{Response to R1}
\subsection{} \textbf{}

\subsection{} \textbf{
  One main concern is that at present the logic in this paper is hard to follow.
  I say this, being very familiar with much of this group's earlier work on
  Rule-Based and Information-Integration learning, though I had not read the
  series of Crossley studies on which the current paper builds. In fact, after
  reading the current manuscript, I went back to look at the Crossley JEP:General
  paper to help me grasp this one. We can't expect readers to do this, though, so
  the logic needs to be spelled out early here in a way that is self-standing from
  the original.
}

\subsection{} \textbf{
  The key concept of feedback contingency is not defined clearly or consistently.
  The abstract states: "feedback contingency, defined as the degree of
  non-randomness between response and outcome, plays a vital role in controlling a
  gate that normally prevents knowledge from being modified during interventions".
  As I read this for the first time, I wondered whether this meant actual feedback
  contingency or detected feedback contingency. The title suggests the latter,
  which seems to be what is actually meant, but that distinction is not stated
  clearly in the abstract, nor the first paragraph of the intro, nor, I think,
  through most of the text. In fact, adding to my confusion, feedback contingency
  is defined in two different ways in this paper, e.g., in the abstract "the
  degree of non-randomness between response and outcome" vs. in the discussion (p.
  15) "the correlation between response confidence and response valence." These
  don't seem equivalent.
}

\subsection{} \textbf{
  Some statements seemed very general and vague and felt like double negatives.
  For example, I had trouble with the abstract's statement that increasing the
  cognitive load, ""should disrupt the accurate estimation of contingency, and
  thereby prevent the gate on procedural learning from closing." It seems it would
  be more direct to end with "thereby keep the learning gate open, so that the
  original habit can be unlearned."
}

\subsection{} \textbf{
  Much of the introduction describes the Crossley paper, and yet it is not clearly
  stated where that is the case (i.e., where the Crossley paper is being described
  vs the present experiment). Just one example of this is at the beginning of the
  last paragraph on the first page of the introduction: on first reading I wasn't
  sure that this paragraph was still referring to Crossley et al. And Figure 3
  shows Crossley data, but the Figure caption doesn't indicate this. Perhaps
  subheadings would help to differentiate the Crossley paper findings from the
  current study.
}

\subsection{} \textbf{
  In addition, right now the second paragraph moves into details of Crossley's
  evidence without making the broader picture clear. The description of how the
  model evolved (in the first few pages of the intro) is difficult to follow,
  because the first-time reader doesn't know where we are headed or why. I think
  the paper needs to begin with a clear, concise, specific statement of the
  underlying hypotheses about unlearning (or overwriting) that are being tested
  here, before going into the details of the evidence in the earlier Crossley work
  that led to these hypotheses.
}

\subsection{} \textbf{
  Another general concern is that the present paper, while offering intriguing
  data, seems to be missing some key components that would provide clearer
  evidence for the conclusions that are being drawn. For example, a key assumption
  underlying the conclusions drawn here is that the original category learning is
  not itself declarative. There is certainly evidence from the earlier work of
  this group that that is the case, but providing direct evidence for that here
  (using the current dual task) would strengthen the paper. Thus, it seems
  important to include a condition in which the dual task occurs throughout
  training; the dual task should not affect the original category learning, even
  though it is influencing contingency detection during the intervention phase. As
  another example of missing conditions, we don't have some of the clearest
  evidence regarding the extent to which unlearning has occurred because the
  present study lacks New Learning groups (of the sort included in the Crossley
  2013 paper). Thus, the present experiments are more like an addendum to the
  Crossley 2013 paper, than a freestanding study, and so they provide a more
  modest increment in evidence than is typically expected for a JEP paper. All of
  this would, perhaps, be one thing if the present paper could be framed as a
  brief report, but given the complexity of the argument, at least as presented in
  this manuscript, this doesn't seem feasible.
}

- frame as brief report
- 20+ years of evidence suggests model fits are reliable indicators of system use
- Dual-task does have an effect on II, just less than RB, so proposed design
would not allow us to clearly distinguish between impairments from simple
increased load, and those derived from impaired contingency estimation.
- New learning conditions would be useful, but (1) to include them for all
tested conditions would mean another 150+ participants (2) they aren't necessary
because Greg: say something smart here.


\subsection{} \textbf{
  Related to point 2 above, as the manuscript indicates (page 14), some of these
  results are unanticipated in light of this group's earlier work, e.g. the lack
  of robust savings in any group here. That is, contrary to the idea (and their
  earlier findings) that random feedback during intervention leads to no
  unlearning (i.e., to savings of the original learning), in fact none of the
  present groups, including the control, showed such savings. Some plausible
  explanations for these unanticipated results are offered, but this situation
  seems to make those missing conditions mentioned in #2 more problematic.
}

- We presented our results in a suboptimal fashion
- There is likely fatgiue at play...
- However, there is still robust evidence for savings (first 50 trials)

\subsection{} \textbf{
  On page 6, is the paragraph concerning the mixed-feedback really necessary here?
  It seems the goal here should be to present only as much of the detail of the
  original Crossley paper as is needed to set up the present study.
}

\subsection{} \textbf{
  On page 9, end of middle paragraph, I think that the trial numbers indicated
  here for condition 4 (400-650) seem to be inconsistent with what was said in the
  middle of page 7?
}

\subsection{} \textbf{
  Bottom of page 11: It is stated that in Fig 5, the red lines are always below
  the blue line during reacquisition. This does not seem to me to be true of Fig
  5D.
}

\subsection{} \textbf{
  Page 14, summary: Contrary to what the third sentence here says, this paper does
  not investigate the "neural mechanisms" in any direct way, so I think better to
  not overstate.
}


\section{Response to R2}

\subsection{} \textbf{
  The introduction is very confusing. There is a lot of material that is explained
  in a very short amount of space. I am not sure why the authors go into so much
  details of Crossley et al. (2013) and why they even include figures of past
  results. I would suggest removing the figures and many details. Also, breaking
  the introduction into subsections might help to streamline the presentation. The
  authors might also cover work that was not performed in their lab.
}

- second recommendation to use subsections
- NOTE: does anybody know of any work on contingency detection not done in our
labs? Perhaps there are some saving references from the motor world I can pull.

\subsection{} \textbf{
  In Fig. 2, what is SPN?
}

\subsection{} \textbf{
  p. 6 "More specifically, when the feedback is random the correlation between
  response confidence and feedback valence is zero." This is true, but there are
  other cases when the correlation between confidence and feedback is zero. For
  example, when one begins a task and has no idea how to proceed. Confidence is
  near 0, and some of the responses are correct (randomly). Why are the TANs not
  closing the gate?
}

\subsection{} \textbf{
  p. 12: The number of df in the analyses are uncharacteristically large. Did the
  authors use each trial as a df instead of averaging the data by subject (e.g.,
  t-tests with over 900 df)?f
}

\subsection{} \textbf{
  p. 12 "The Condition × Block interaction was also significant [F (4, 2598) =
  14.64, p < 0.001, Ω 2 = 0.02], reflecting the slower change in performance in
  the dual-task conditions relative to the no dual-task control." The authors
  would need to decompose the interaction in order to make that claim.
}

\subsection{} \textbf{
  p. 14 "...whereas savings in the other dual-task
  conditions were all marginally less than in the control condition..." The p
  values are all >= .1, and some even > .2. It is misleading to consider them as
  "marginally" less. As the authors mentioned in Footnote 1, these tests should
  also all be corrected for multiple comparison -- so your significant result
  might also not be significant.
}

\subsection{} \textbf{
  p. 14: I would suggest removing the regression model since it is not bringing a
  new understanding of the data and further increases the number of statistical
  tests on the same data.
}

\subsection{} \textbf{
  p. 15: "Specifically, our goal was to determine whether prefrontal-based
  declarative memory mechanisms mediate contingency estimation." Similar claims
  are also made on p. 19: "Our results indicate that prefrontal networks likely do
  play an important role in controlling the estimation of feedback contingency,
  and therefore may provide an accessible cortical target for electrical or
  magnetic intervention." The experiment supports the hypothesis that estimating
  contingencies depends on executive functions (or a declarative mechanism), but
  there is no direct test of the role of the PFC in the estimation of
  contingencies so I suggest removing these claims from the paper.
}

\section{Response to R3}

\subsection{} \textbf{
  I did not have significant concerns with the experiment. The primary
  manipulation (use of a numerical Stroop task) is appropriate. The statistical
  analyses are appropriate and complete.
}

\subsection{} \textbf{
  I thought the Introduction was particularly well written: the authors
  successfully laid out the logic of the study, which required integration across
  neural, computational, and prior behavioral work.
}

\subsection{} \textbf{
  My main concerns are with the Discussion. It lacked a clear conclusion and
  "take-home" message for the reader. In addition most of the discussion was taken
  up by two somewhat tangential topics (lack of savings and the procedural nature
  of category learning). The discussion should be broadened, and the treatment of
  these two topics condensed.
}


\bibliography{sample}

\end{document}